{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_evidences1(evidences, json_data):\n",
    "    \"\"\"\n",
    "    Parse evidences into meaningful text using JSON mappings.\n",
    "    \"\"\"\n",
    "    parsed = []\n",
    "    for evidence in eval(evidences):  # Convert string list to actual list\n",
    "        if \"_@_\" in evidence:\n",
    "            code, value = evidence.split(\"_@_\")\n",
    "            question = json_data.get(code, {}).get('question_en', 'Unknown question')\n",
    "            value_meaning = json_data.get(code, {}).get('value_meaning', {}).get(value, {}).get('en', value)\n",
    "            parsed.append(f\"{question} - {value_meaning}\")\n",
    "        else:\n",
    "            question = json_data.get(evidence, {}).get('question_en', 'Unknown question')\n",
    "            parsed.append(f\"{question} - Y\")\n",
    "    return parsed\n",
    "\n",
    "def parse_evidences2(evidences, json_data):\n",
    "    \"\"\"\n",
    "    Parse evidences into meaningful text using JSON mappings and combine repeated questions' answers with &.\n",
    "    \"\"\"\n",
    "    parsed = {}\n",
    "    for evidence in eval(evidences):  # Convert string list to actual list\n",
    "        if \"_@_\" in evidence:\n",
    "            code, value = evidence.split(\"_@_\")\n",
    "            question = json_data.get(code, {}).get('question_en', 'Unknown question')\n",
    "            value_meaning = json_data.get(code, {}).get('value_meaning', {}).get(value, {}).get('en', value)\n",
    "            if question in parsed:\n",
    "                parsed[question] += f\" & {value_meaning}\"\n",
    "            else:\n",
    "                parsed[question] = value_meaning\n",
    "        else:\n",
    "            question = json_data.get(evidence, {}).get('question_en', 'Unknown question')\n",
    "            if question in parsed:\n",
    "                parsed[question] += \" & Y\"\n",
    "            else:\n",
    "                parsed[question] = \"Y\"\n",
    "    \n",
    "    return [f\"{q} - {a}\" for q, a in parsed.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(csv_path, json_path, output_path):\n",
    "    \"\"\"\n",
    "    Transforms the CSV and JSON data into BioBERT-friendly format.\n",
    "    \"\"\"\n",
    "    # Load JSON and CSV files\n",
    "    with open(json_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "    \n",
    "    csv_data = pd.read_csv(csv_path)\n",
    "\n",
    "    # Process each row in the CSV\n",
    "    formatted_data = []\n",
    "\n",
    "    for _, row in csv_data.iterrows():\n",
    "        patient_data = {\n",
    "            \"Age\": row['AGE'],\n",
    "            \"Sex\": row['SEX'],\n",
    "            \"Antecedents\": [],\n",
    "            \"Symptoms\": [],\n",
    "            \"Differential Diagnosis\": [],\n",
    "        }\n",
    "\n",
    "        # Parse evidences\n",
    "        evidences = parse_evidences2(row['EVIDENCES'], json_data)\n",
    "        for evidence in evidences:\n",
    "            if \"Antecedent\" in evidence:  # Example categorization logic\n",
    "                patient_data[\"Antecedents\"].append(evidence)\n",
    "            else:\n",
    "                patient_data[\"Symptoms\"].append(evidence)\n",
    "\n",
    "        # Parse differential diagnosis (exclude probabilities)\n",
    "        diagnoses = [diag[0] for diag in eval(row['DIFFERENTIAL_DIAGNOSIS'])]\n",
    "        patient_data[\"Differential Diagnosis\"] = diagnoses\n",
    "\n",
    "        formatted_data.append(patient_data)\n",
    "\n",
    "    # Save the formatted data to a JSON file\n",
    "    with open(output_path, 'w') as output_file:\n",
    "        json.dump(formatted_data, output_file, indent=4)\n",
    "\n",
    "task = \"train\"\n",
    "data = pd.read_csv(f'dataset/{task}.csv')\n",
    "data.sample(n=2000).to_csv(f'dataset/samples/{task}_sample50.csv', index=False)\n",
    "\n",
    "# Paths to input files and output location\n",
    "csv_path = f'dataset/samples/{task}_sample50.csv'  # Replace with the actual CSV file path\n",
    "json_path = 'release_evidences_cleaned.json'  # Replace with the actual JSON file path\n",
    "output_path = f'dataset_processed/{task}_sample50.json'  # Desired output file name\n",
    "\n",
    "# Transform the data\n",
    "transform_data(csv_path, json_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioBERTDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize input text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_model(texts, labels, model_name=\"dmis-lab/biobert-v1.1\", max_length=128, batch_size=16, epochs=3, lr=2e-5):\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "    # Load tokenizer and dataset\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    dataset = BioBERTDataset(texts, labels, tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Load model\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))\n",
    "    model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    num_training_steps = len(dataloader) * epochs\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(dataloader, leave=True)\n",
    "        for batch in loop:\n",
    "            input_ids = batch[\"input_ids\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "            attention_mask = batch[\"attention_mask\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "            labels = batch[\"label\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            loop.set_description(f\"Epoch {epoch}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    return model, tokenizer, label_encoder\n",
    "\n",
    "# Load preprocessed data\n",
    "with open(\"transformed_data.json\", \"r\") as data_file:\n",
    "    data = json.load(data_file)\n",
    "\n",
    "texts = [\" \".join(item[\"Symptoms\"]) for item in data]\n",
    "labels = [item[\"Differential Diagnosis\"][0] for item in data]  # Primary diagnosis as target\n",
    "\n",
    "# Train BioBERT model\n",
    "model, tokenizer, label_encoder = train_model(texts, labels)\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"biobert_diagnosis_model\")\n",
    "tokenizer.save_pretrained(\"biobert_diagnosis_model\")\n",
    "\n",
    "# Save label encoder\n",
    "with open(\"label_encoder.json\", \"w\") as le_file:\n",
    "    json.dump(label_encoder.classes_.tolist(), le_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With validation set\n",
    "class BioBERTDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize input text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_model(train_texts, train_labels, val_texts, val_labels, model_path=\"dmis-lab/biobert-v1.1\", max_length=128, batch_size=16, epochs=3, lr=2e-5):\n",
    "    # Load tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # Encode labels\n",
    "    unique_labels = sorted(set(train_labels + val_labels))\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    train_labels = [label_to_idx[label] for label in train_labels]\n",
    "    val_labels = [label_to_idx[label] for label in val_labels]\n",
    "\n",
    "    # Prepare datasets and dataloaders\n",
    "    train_dataset = BioBERTDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "    val_dataset = BioBERTDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Load model\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(unique_labels))\n",
    "    model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    num_training_steps = len(train_loader) * epochs\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    # Training and validation loop\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "            attention_mask = batch[\"attention_mask\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "            labels = batch[\"label\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "                input_ids = batch[\"input_ids\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "                attention_mask = batch[\"attention_mask\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "                labels = batch[\"label\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                val_predictions.extend(predictions)\n",
    "                val_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average=\"weighted\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model.save_pretrained(\"best_biobert_model\")\n",
    "            tokenizer.save_pretrained(\"best_biobert_model\")\n",
    "            print(\"Best model saved!\")\n",
    "\n",
    "    return model, tokenizer, label_to_idx\n",
    "\n",
    "# Load training and validation data\n",
    "with open(\"train_data.json\", \"r\") as train_file:\n",
    "    train_data = json.load(train_file)\n",
    "with open(\"val_data.json\", \"r\") as val_file:\n",
    "    val_data = json.load(val_file)\n",
    "\n",
    "train_texts = [\" \".join(item[\"Symptoms\"]) for item in train_data]\n",
    "train_labels = [item[\"Differential Diagnosis\"][0] for item in train_data]\n",
    "val_texts = [\" \".join(item[\"Symptoms\"]) for item in val_data]\n",
    "val_labels = [item[\"Differential Diagnosis\"][0] for item in val_data]\n",
    "\n",
    "# Train the model\n",
    "train_model(train_texts, train_labels, val_texts, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioBERTDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        # Tokenize input text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "def evaluate_model(test_texts, model_path=\"biobert_diagnosis_model\", max_length=128, batch_size=16):\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare dataset and dataloader\n",
    "    dataset = BioBERTDataset(test_texts, tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # Inference loop\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "            attention_mask = batch[\"attention_mask\"].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            predictions.extend(batch_predictions)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Load test data\n",
    "with open(\"test_data.json\", \"r\") as test_file:\n",
    "    test_data = json.load(test_file)\n",
    "\n",
    "test_texts = [\" \".join(item[\"Symptoms\"]) for item in test_data]\n",
    "\n",
    "def calculate_metrics(predictions, true_labels):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions, average=\"weighted\")\n",
    "    return accuracy, f1\n",
    "\n",
    "# Load label encoder\n",
    "with open(\"label_encoder.json\", \"r\") as le_file:\n",
    "    label_classes = json.load(le_file)\n",
    "\n",
    "# Assuming true_labels.json contains ground truth labels for the test set\n",
    "with open(\"true_labels.json\", \"r\") as true_file:\n",
    "    true_labels_raw = json.load(true_file)\n",
    "\n",
    "# Encode true labels\n",
    "true_labels = [label_classes.index(label) for label in true_labels_raw]\n",
    "\n",
    "# Evaluate model\n",
    "predictions = evaluate_model(test_texts)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy, f1 = calculate_metrics(predictions, true_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
